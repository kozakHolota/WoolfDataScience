{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Визначте DataFrame з тривимірними векторами слів",
   "id": "d016cd8b462d0430"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-20T19:57:02.900037Z",
     "start_time": "2025-09-20T19:57:02.894695Z"
    }
   },
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "word_embeddings = Path.cwd() / \"data\" / \"word_embeddings_subset.p\"\n",
    "\n",
    "with word_embeddings.open(\"rb\") as file:\n",
    "    word_embeddings = pickle.load(file)"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T19:57:02.909415Z",
     "start_time": "2025-09-20T19:57:02.907033Z"
    }
   },
   "cell_type": "code",
   "source": "word_embeddings.keys()",
   "id": "8c84506f6966f401",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['country', 'city', 'China', 'Iraq', 'oil', 'town', 'Canada', 'London', 'England', 'Australia', 'Japan', 'Pakistan', 'Iran', 'gas', 'happy', 'Russia', 'Afghanistan', 'France', 'Germany', 'Georgia', 'Baghdad', 'village', 'Spain', 'Italy', 'Beijing', 'Jordan', 'Paris', 'Ireland', 'Turkey', 'Egypt', 'Lebanon', 'Taiwan', 'Tokyo', 'Nigeria', 'Vietnam', 'Moscow', 'Greece', 'Indonesia', 'sad', 'Syria', 'Thailand', 'Libya', 'Zimbabwe', 'Cuba', 'Ottawa', 'Tehran', 'Sudan', 'Kenya', 'Philippines', 'Sweden', 'Poland', 'Ukraine', 'Rome', 'Venezuela', 'Switzerland', 'Berlin', 'Bangladesh', 'Portugal', 'Ghana', 'Athens', 'king', 'Madrid', 'Somalia', 'Dublin', 'Qatar', 'Chile', 'Islamabad', 'Bahrain', 'Nepal', 'Norway', 'Serbia', 'Kabul', 'continent', 'Brussels', 'Belgium', 'Uganda', 'petroleum', 'Cairo', 'Denmark', 'Austria', 'Jamaica', 'Georgetown', 'Bangkok', 'Finland', 'Peru', 'Romania', 'Bulgaria', 'Hungary', 'Vienna', 'Kingston', 'Manila', 'Cyprus', 'Azerbaijan', 'Copenhagen', 'Fiji', 'Tunisia', 'Kazakhstan', 'queen', 'Beirut', 'Jakarta', 'Croatia', 'Belarus', 'Algeria', 'Malta', 'Morocco', 'Rwanda', 'Bahamas', 'Damascus', 'Ecuador', 'Angola', 'Canberra', 'Liberia', 'Honduras', 'Tripoli', 'Slovakia', 'Doha', 'Armenia', 'Taipei', 'Oman', 'Nairobi', 'Santiago', 'Guinea', 'Uruguay', 'Stockholm', 'Slovenia', 'Zambia', 'Havana', 'Uzbekistan', 'Belgrade', 'Mogadishu', 'Khartoum', 'Botswana', 'Kyrgyzstan', 'Dhaka', 'Namibia', 'Ankara', 'Abuja', 'Lima', 'Harare', 'Warsaw', 'Malawi', 'Lisbon', 'Latvia', 'Niger', 'Lithuania', 'Estonia', 'Samoa', 'Oslo', 'Nicaragua', 'Hanoi', 'Sofia', 'Macedonia', 'Senegal', 'Mozambique', 'Guyana', 'Mali', 'Accra', 'Kathmandu', 'Tbilisi', 'Helsinki', 'Montenegro', 'Caracas', 'Laos', 'Budapest', 'Kiev', 'Turkmenistan', 'Eritrea', 'Albania', 'Madagascar', 'Nassau', 'Kampala', 'Amman', 'Greenland', 'Belize', 'Moldova', 'Burundi', 'Tajikistan', 'Baku', 'Astana', 'Gambia', 'Bucharest', 'joyful', 'Monrovia', 'Mauritania', 'Algiers', 'Muscat', 'Bern', 'Luanda', 'Dakar', 'Tunis', 'Gabon', 'Minsk', 'Liechtenstein', 'Suva', 'Yerevan', 'Zagreb', 'Bishkek', 'Manama', 'Kigali', 'Riga', 'Lusaka', 'Tashkent', 'Nicosia', 'Valletta', 'Windhoek', 'Dominica', 'Quito', 'Tallinn', 'Bratislava', 'Tegucigalpa', 'Skopje', 'Gaborone', 'Rabat', 'Maputo', 'Suriname', 'Vilnius', 'Montevideo', 'Ljubljana', 'Tirana', 'Dushanbe', 'Ashgabat', 'Asmara', 'Tuvalu', 'Managua', 'Conakry', 'Banjul', 'Bamako', 'Lilongwe', 'Vientiane', 'Chisinau', 'Roseau', 'Nouakchott', 'Podgorica', 'Niamey', 'Bujumbura', 'Apia', 'Antananarivo', 'Libreville', 'Belmopan', 'Vaduz', 'Paramaribo', 'Nuuk', 'Funafuti'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Визначте функції для пошуку найближчого слова",
   "id": "a342c7a92fdc9516"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T19:57:02.925521Z",
     "start_time": "2025-09-20T19:57:02.917865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def return_3d(arr: np.ndarray) -> np.ndarray:\n",
    "    # Гарантуємо 2D float масив\n",
    "    A = np.asarray(arr, dtype=np.float64)\n",
    "    if A.ndim != 2:\n",
    "        raise ValueError(f\"PCA очікує 2D масив (n_samples, n_features), отримано shape={A.shape}\")\n",
    "    n_samples, n_features = A.shape\n",
    "    if n_samples < 1 or n_features < 1:\n",
    "        raise ValueError(\"Порожній масив для PCA.\")\n",
    "    n_components = min(3, n_samples, n_features)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(A)\n",
    "\n",
    "def dict_to_db(dct: dict) -> pd.DataFrame:\n",
    "    # Перетворюємо словник {слово: вектор} у 2D числовий масив\n",
    "    if not isinstance(dct, dict) or len(dct) == 0:\n",
    "        raise ValueError(\"Очікується непорожній словник ембеддінгів.\")\n",
    "    keys = list(dct.keys())\n",
    "    vecs = [np.asarray(v, dtype=np.float64).ravel() for v in dct.values()]\n",
    "    lengths = {v.size for v in vecs}\n",
    "    if len(lengths) != 1:\n",
    "        raise ValueError(f\"Всі вектори мають бути однакової довжини, отримано довжини: {sorted(lengths)}\")\n",
    "    X = np.stack(vecs, axis=0)  # (n_samples, n_features)\n",
    "    Xp = return_3d(X)\n",
    "    cols = [f\"pc{i+1}\" for i in range(Xp.shape[1])]\n",
    "    return pd.DataFrame(data=Xp, index=keys, columns=cols)\n",
    "\n",
    "def vec(word: str, embeddings: pd.DataFrame) -> np.ndarray:\n",
    "    try:\n",
    "        return embeddings.loc[word].to_numpy(dtype=np.float64)\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Слова '{word}' немає в ембеддінгах (індексі DataFrame).\") from e\n",
    "\n",
    "def _row_normalize(M: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    # L2-нормалізація рядків\n",
    "    norms = np.linalg.norm(M, axis=1, keepdims=True)\n",
    "    norms = np.maximum(norms, eps)\n",
    "    return M / norms\n",
    "\n",
    "def find_closest_word(df: pd.DataFrame, word_vec: np.ndarray, exclude: set[str] | None = None):\n",
    "    \"\"\"\n",
    "    Шукає найближче слово за косинусною подібністю у 3D-просторі df.\n",
    "    exclude – множина слів, які не можна повертати (напр., початкові).\n",
    "    \"\"\"\n",
    "    if exclude is None:\n",
    "        exclude = set()\n",
    "    # Матриця усіх векторів (n_words, n_dims)\n",
    "    M = df.to_numpy(dtype=np.float64)\n",
    "    # Нормалізуємо матрицю та запит\n",
    "    U = _row_normalize(M)\n",
    "    q = np.asarray(word_vec, dtype=np.float64).ravel()\n",
    "    q_norm = q / max(np.linalg.norm(q), 1e-12)\n",
    "    # Косинусні подібності\n",
    "    sims = U @ q_norm  # (n_words,)\n",
    "    # Відкинути заборонені індекси\n",
    "    mask = np.ones(len(df.index), dtype=bool)\n",
    "    if exclude:\n",
    "        exclude_idx = [i for i, w in enumerate(df.index) if w in exclude]\n",
    "        mask[np.array(exclude_idx, dtype=int)] = False\n",
    "    # argmax по дозволених\n",
    "    allowed_sims = np.where(mask, sims, -np.inf)\n",
    "    i = int(np.argmax(allowed_sims))\n",
    "    return df.index[i]\n",
    "\n",
    "def find_topk_words(df: pd.DataFrame, word_vec: np.ndarray, k: int = 5, exclude: set[str] | None = None):\n",
    "    \"\"\"\n",
    "    Повертає top-k найближчих слів за косинусом.\n",
    "    \"\"\"\n",
    "    if exclude is None:\n",
    "        exclude = set()\n",
    "    M = df.to_numpy(dtype=np.float64)\n",
    "    U = _row_normalize(M)\n",
    "    q = np.asarray(word_vec, dtype=np.float64).ravel()\n",
    "    q_norm = q / max(np.linalg.norm(q), 1e-12)\n",
    "    sims = U @ q_norm\n",
    "    # Відкидаємо exclude\n",
    "    pairs = [(w, s) for w, s in zip(df.index.tolist(), sims.tolist()) if w not in exclude]\n",
    "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return pairs[:k]"
   ],
   "id": "ac8dce4bbdd4fb18",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Деякі приклади використання функції пошуку",
   "id": "38202d8222876cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:01:31.545992Z",
     "start_time": "2025-09-20T20:01:31.533671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = dict_to_db(word_embeddings)\n",
    "# Класична аналогія: capital_offset = Paris - France\n",
    "capital = vec('Paris', df) - vec('France', df)\n",
    "# Приклад: China + (Paris - France) ≈ Beijing\n",
    "target = vec('China', df) + capital\n",
    "print(find_closest_word(df=df, word_vec=target, exclude={'China', 'Paris', 'France'}))\n",
    "# Також подивимось top-5\n",
    "print(find_topk_words(df=df, word_vec=target, k=5, exclude={'China', 'Paris', 'France'}))"
   ],
   "id": "24dbd259a46cd638",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "[('London', 0.9929758981539933), ('Tokyo', 0.9780659847333701), ('Montevideo', 0.9683353580080912), ('city', 0.9645896406653922), ('Dublin', 0.96246823502562)]\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:02:38.531682Z",
     "start_time": "2025-09-20T20:02:38.528897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Приклад: England + (Paris - France) ≈ London\n",
    "target = vec('England', df) + capital\n",
    "print(find_closest_word(df=df, word_vec=target, exclude={'China', 'Paris', 'France'}))\n",
    "# Також подивимось top-5\n",
    "print(find_topk_words(df=df, word_vec=target, k=5, exclude={'China', 'Paris', 'France'}))"
   ],
   "id": "13280900a02db360",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "[('London', 0.9929758981539933), ('Tokyo', 0.9780659847333701), ('Montevideo', 0.9683353580080912), ('city', 0.9645896406653922), ('Dublin', 0.96246823502562)]\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:04:02.243567Z",
     "start_time": "2025-09-20T20:04:02.240752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target = vec(\"oil\", df) - vec(\"king\", df)\n",
    "\n",
    "print(find_closest_word(df=df, word_vec=target, exclude={'China', 'Paris', 'France'}))\n",
    "# Також подивимось top-5\n",
    "print(find_topk_words(df=df, word_vec=target, k=5, exclude={'China', 'Paris', 'France'}))"
   ],
   "id": "bc2615888af754ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kyrgyzstan\n",
      "[('Kyrgyzstan', 0.9014647405248613), ('Tajikistan', 0.8970380281526311), ('Eritrea', 0.8606097712180828), ('Turkmenistan', 0.8231562406378717), ('Burundi', 0.7646099109258514)]\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Обчисліть векторний добуток для знаходження ортогонального слова",
   "id": "2789c2c4c811b0dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:21:10.248810Z",
     "start_time": "2025-09-20T20:21:10.244567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ort1 = np.cross(vec(\"oil\", df), vec(\"king\", df))\n",
    "\n",
    "print(find_closest_word(df=df, word_vec=ort1, exclude={'oil', 'king'}))\n",
    "# Також подивимось top-5\n",
    "print(find_topk_words(df=df, word_vec=ort1, k=5, exclude={'oil', 'king'}))"
   ],
   "id": "9245857179e061b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georgia\n",
      "[('Georgia', 0.9443204394917163), ('Estonia', 0.9423309900125145), ('Lithuania', 0.9401424418884858), ('Poland', 0.9377380907370603), ('Hungary', 0.9258560318606222)]\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:22:14.528378Z",
     "start_time": "2025-09-20T20:22:14.525577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ort2 = np.cross(vec(\"France\", df), vec(\"Paris\", df))\n",
    "\n",
    "print(find_closest_word(df=df, word_vec=ort2, exclude={'France', 'Paris'}))\n",
    "# Також подивимось top-5\n",
    "print(find_topk_words(df=df, word_vec=ort2, k=5, exclude={'France', 'Paris'}))"
   ],
   "id": "3ef0384c8ccc1c52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zagreb\n",
      "[('Zagreb', 0.9210633044760501), ('Skopje', 0.9074607442877155), ('Moscow', 0.8962049551696625), ('Vaduz', 0.8927146550044899), ('Riga', 0.8890936701109639)]\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Напишіть функції визначення кута між словами",
   "id": "9c0302e10d4b246c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:46:58.792595Z",
     "start_time": "2025-09-20T20:46:58.789988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def vec_from_dict(word: str, embeddings: dict[str, np.ndarray]) -> np.ndarray:\n",
    "    return embeddings[word]\n",
    "\n",
    "def angle_between(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    u = np.asarray(u, dtype=np.float64).ravel()\n",
    "    v = np.asarray(v, dtype=np.float64).ravel()\n",
    "    nu = np.linalg.norm(u)\n",
    "    nv = np.linalg.norm(v)\n",
    "    if nu == 0 or nv == 0:\n",
    "        raise ValueError(\"Неможливо обчислити кут для нульового вектора.\")\n",
    "    cos_theta = float(np.dot(u, v) / (nu * nv))\n",
    "    # Числова стабільність\n",
    "    cos_theta = max(-1.0, min(1.0, cos_theta))\n",
    "    theta = np.arccos(cos_theta)\n",
    "    return float(np.degrees(theta))"
   ],
   "id": "21f4270868b8bb80",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Використаємо функцію",
   "id": "372e77430cbb0f58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:47:01.009601Z",
     "start_time": "2025-09-20T20:47:01.007035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "country = vec_from_dict(\"France\", word_embeddings)\n",
    "capital = vec_from_dict(\"Paris\", word_embeddings)\n",
    "\n",
    "print(angle_between(country, capital))"
   ],
   "id": "5da4ffda7e571c8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.69184994088394\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:47:31.789447Z",
     "start_time": "2025-09-20T20:47:31.786797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "oil = vec_from_dict(\"oil\", word_embeddings)\n",
    "king = vec_from_dict(\"king\", word_embeddings)\n",
    "print(angle_between(oil, king))"
   ],
   "id": "f9ae818a101851a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.48644951540895\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:49:03.092049Z",
     "start_time": "2025-09-20T20:49:03.055321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "france = vec_from_dict(\"France\", word_embeddings)\n",
    "england = vec_from_dict('England', word_embeddings)\n",
    "\n",
    "print(angle_between(france, england))"
   ],
   "id": "818613dc953a109a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.54370256990977\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Висновки щодо кутів між словами\n",
    "\n",
    "З практичних прикладів випливає, що чим ближчі слова одне до одного, тим менший кут між ними. Із загального: стиснення знижує точність знаходження як найближчого слова, так і кутів між словами."
   ],
   "id": "acaad0d76a0ab93e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
